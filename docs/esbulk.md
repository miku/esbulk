ESBULK 1 "JANUAR 2018" "Leipzig University Library" "Manuals"
=============================================================

NAME
----

esbulk - send documents to elasticsearch in bulk and in parallel.

SYNOPSIS
--------

`esbulk` [`-server` *URL*, `-index` *name*, `-size` *N*, `-w` *N*, `-z`] < *file*

DESCRIPTION
-----------

esbulk takes as input a newline delimited JSON file and indexes all documents
into elasticsearch running on a given server address. The documents are batched
and indexed in parallel to achieve a high indexing throughput.

The newline delimited JSON text file format is explained at http://jsonlines.org/ and http://ndjson.org/.

OPTIONS
-------

`-0`
  Set the number of replicas to 0 during indexing (this can speed up indexing significantly, the original value is restored at the end and may cause delay until the cluster is green).

`-c` *string*
  Create index mappings, settings, aliases, https://is.gd/3zszeu.

`-cpuprofile` *string*
  Write cpu profile to file.

`-id` *string*
  Reuse value from this field as id. By default ids are autogenerated.

`-index` *string*
  Index name.

`-mapping` *filename*
  Mapping string or filename to apply before indexing.

`-memprofile` *string*
  Write heap profile to file.

`-optype` *string*
  optype (index - will replace existing data, create - will only create a new doc,
  update - create new or update existing data) (default "index")

`-p` *name*
  Pipeline to use to preprocess documents.

`-purge`
  Purge any existing index before reindexing. Warning: No confirmation required.

`-r string`
  Refresh interval after import (default "1s")

`-server` *URL*
  Server hostport including schema like http://localhost:9200

`-size` *N*
  Batch size. Defaults to 1000. Increase for small documents.

`-skipbroken`
  Skip broken json.

`-type` *string*
  Elasticsearch type (deprecated in 6.0.0, https://is.gd/HFsOWt), empty string.

`-u` *string*
  HTTP basic authentication "username:password" (like curl -u).

`-v`
  Program version.

`-verbose`
  Show progress.

`-w` *N*
  Number of workers. Defaults to number of cores.

`-z`
  Decompress gzip input file on the fly.

EXAMPLES
--------

Index a compressed file:

  `esbulk -index abc -verbose -server 110.81.131.200:9200 -z file.ldj.gz`

Index from standard input:

  `cat file.ldj | esbulk -index abc -server 110.81.131.200:9200`

Purge an existing index, apply a mapping from a file and index:

  `esbulk -purge -mapping mapping.json -index abc file.ldj`

DIAGNOSITCS
-----------

If indexing pressure on the bulk API is too high (dozens or hundreds of
parallel workers, large batch sizes, depending on you setup), esbulk will halt
and report an error:

```
$ esbulk -index my-index-name -w 100 file.ldj
2017/01/02 16:25:25 error during bulk operation, try less workers (lower -w value) or
increase thread_pool.bulk.queue_size in your nodes
```

Please note that, in such a case, some documents are indexed and some are not.
Your index will be in an INCONSISTENT STATE, since there is no transactional
bracket around the indexing process.

However, using defaults (parallism: number of cores) on a single node setup
will just work. For larger clusters, increase the number of workers until you
see full CPU utilization. After that, more workers won't buy any more speed.

BUGS
----

Please report bugs to https://github.com/miku/esbulk/issues.

AUTHORS
------

Martin Czygan <https://github.com/miku>, <martin.czygan@uni-leipzig.de>
sakshambathla <https://github.com/sakshambathla>
Klaubert Herr <https://github.com/klaubert>
Yusuke KUOKA <https://github.com/mumoshu>
faultlin3 <https://github.com/faultlin3>
gransy <https://github.com/gransy>
Christoph Kepper <https://github.com/ckepper>
Christian Solomon
Mikael Bystr√∂m

SEE ALSO
--------

[FINC](https://finc.info), [AMSL](http://amsl.technology/), [solrbulk](https://github.com/miku/solrbulk)

